{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape before oversampling:  (1489, 3)  Train set shape after resampling:  (2922, 3)\n",
      "Found 2922 non-validated image filenames belonging to 2 classes.\n",
      "Found 146 non-validated image filenames belonging to 2 classes.\n",
      "Found 182 non-validated image filenames belonging to 2 classes.\n",
      "Training distribution:  ['Class COVID-19: 1461. ', 'Class non-COVID-19: 1461. ']\n",
      "MODEL CONFIG:  {'KERNEL_SIZE': '(3,3)', 'STRIDES': '(1,1)', 'INIT_FILTERS': 16, 'FILTER_EXP_BASE': 3, 'MAXPOOL_SIZE': '(2,2)', 'CONV_BLOCKS': 3, 'NODES_DENSE0': 128, 'LR': 1e-05, 'OPTIMIZER': 'adam', 'DROPOUT': 0.4, 'L2_LAMBDA': 0.0001}\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_0 (Conv2D)                (None, 224, 224, 16) 448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 224, 224, 16) 64          conv0_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 224, 224, 16) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv0_1 (Conv2D)                (None, 224, 224, 16) 2320        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concat0 (Concatenate)           (None, 224, 224, 19) 0           conv0_1[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 224, 224, 19) 76          concat0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 224, 224, 19) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 112, 112, 19) 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1_0 (Conv2D)                (None, 112, 112, 48) 8256        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 112, 112, 48) 192         conv1_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 112, 112, 48) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1 (Conv2D)                (None, 112, 112, 48) 20784       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concat1 (Concatenate)           (None, 112, 112, 67) 0           conv1_1[0][0]                    \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 112, 112, 67) 268         concat1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 112, 112, 67) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 67)   0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_0 (Conv2D)                (None, 56, 56, 144)  86976       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 56, 56, 144)  576         conv2_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 56, 56, 144)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1 (Conv2D)                (None, 56, 56, 144)  186768      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concat2 (Concatenate)           (None, 56, 56, 211)  0           conv2_1[0][0]                    \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 56, 56, 211)  844         concat2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 56, 56, 211)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 211)  0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 165424)       0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 165424)       0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          21174400    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output (Activation)             (None, 2)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,482,230\n",
      "Trainable params: 21,481,220\n",
      "Non-trainable params: 1,010\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-1-db34cc146bdb>:142: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/200\n",
      " 1/92 [..............................] - ETA: 0s - loss: 669.8630 - accuracy: 0.5312 - precision: 0.6000 - recall: 0.3529 - auc: 0.5176 - f1score: 0.4444WARNING:tensorflow:From C:\\Users\\PaulDS3\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 81s 885ms/step - loss: 629.3560 - accuracy: 0.7553 - precision: 0.7541 - recall: 0.7577 - auc: 0.8240 - f1score: 0.7559 - val_loss: 498.0530 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9885 - val_f1score: 0.0000e+00\n",
      "Epoch 2/200\n",
      "92/92 [==============================] - 80s 872ms/step - loss: 569.9673 - accuracy: 0.9261 - precision: 0.9192 - recall: 0.9343 - auc: 0.9749 - f1score: 0.9267 - val_loss: 451.2234 - val_accuracy: 0.9795 - val_precision: 0.5000 - val_recall: 0.3333 - val_auc: 0.9907 - val_f1score: 0.4000\n",
      "Epoch 3/200\n",
      "92/92 [==============================] - 81s 884ms/step - loss: 527.2706 - accuracy: 0.9565 - precision: 0.9483 - recall: 0.9658 - auc: 0.9907 - f1score: 0.9569 - val_loss: 453.9994 - val_accuracy: 0.9521 - val_precision: 0.1667 - val_recall: 0.3333 - val_auc: 0.9798 - val_f1score: 0.2222\n",
      "Epoch 4/200\n",
      "92/92 [==============================] - 89s 970ms/step - loss: 493.7624 - accuracy: 0.9784 - precision: 0.9736 - recall: 0.9836 - auc: 0.9967 - f1score: 0.9785 - val_loss: 460.9189 - val_accuracy: 0.7945 - val_precision: 0.0645 - val_recall: 0.6667 - val_auc: 0.8907 - val_f1score: 0.1176\n",
      "Epoch 5/200\n",
      "92/92 [==============================] - 84s 911ms/step - loss: 465.8022 - accuracy: 0.9682 - precision: 0.9659 - recall: 0.9706 - auc: 0.9952 - f1score: 0.9682 - val_loss: 457.0923 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9878 - val_f1score: 0.0000e+00\n",
      "Epoch 6/200\n",
      "92/92 [==============================] - 84s 914ms/step - loss: 440.8409 - accuracy: 0.9843 - precision: 0.9803 - recall: 0.9884 - auc: 0.9984 - f1score: 0.9843 - val_loss: 446.1946 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9879 - val_f1score: 0.0000e+00\n",
      "Epoch 7/200\n",
      "92/92 [==============================] - 83s 904ms/step - loss: 418.6313 - accuracy: 0.9843 - precision: 0.9816 - recall: 0.9870 - auc: 0.9986 - f1score: 0.9843 - val_loss: 429.6746 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9823 - val_f1score: 0.0000e+00\n",
      "Epoch 8/200\n",
      "92/92 [==============================] - 83s 905ms/step - loss: 398.8506 - accuracy: 0.9853 - precision: 0.9836 - recall: 0.9870 - auc: 0.9985 - f1score: 0.9853 - val_loss: 411.9087 - val_accuracy: 0.9041 - val_precision: 0.1333 - val_recall: 0.6667 - val_auc: 0.9696 - val_f1score: 0.2222\n",
      "Epoch 9/200\n",
      "92/92 [==============================] - 84s 909ms/step - loss: 381.4664 - accuracy: 0.9867 - precision: 0.9843 - recall: 0.9890 - auc: 0.9987 - f1score: 0.9867 - val_loss: 395.0317 - val_accuracy: 0.9247 - val_precision: 0.1000 - val_recall: 0.3333 - val_auc: 0.9777 - val_f1score: 0.1538\n",
      "Epoch 10/200\n",
      "92/92 [==============================] - 84s 918ms/step - loss: 365.9041 - accuracy: 0.9839 - precision: 0.9810 - recall: 0.9870 - auc: 0.9976 - f1score: 0.9840 - val_loss: 380.1956 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9792 - val_f1score: 0.0000e+00\n",
      "Epoch 11/200\n",
      "92/92 [==============================] - 84s 912ms/step - loss: 352.1051 - accuracy: 0.9784 - precision: 0.9755 - recall: 0.9815 - auc: 0.9963 - f1score: 0.9785 - val_loss: 366.2724 - val_accuracy: 0.9452 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9802 - val_f1score: 0.0000e+00\n",
      "Epoch 12/200\n",
      "92/92 [==============================] - 84s 910ms/step - loss: 339.3208 - accuracy: 0.9914 - precision: 0.9904 - recall: 0.9925 - auc: 0.9993 - f1score: 0.9915 - val_loss: 353.2981 - val_accuracy: 0.9589 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9841 - val_f1score: 0.0000e+00\n",
      "Epoch 13/200\n",
      "92/92 [==============================] - 84s 914ms/step - loss: 327.8028 - accuracy: 0.9911 - precision: 0.9911 - recall: 0.9911 - auc: 0.9997 - f1score: 0.9911 - val_loss: 341.6367 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9820 - val_f1score: 0.0000e+00\n",
      "Epoch 14/200\n",
      "92/92 [==============================] - 84s 909ms/step - loss: 317.1794 - accuracy: 0.9812 - precision: 0.9808 - recall: 0.9815 - auc: 0.9971 - f1score: 0.9812 - val_loss: 331.0241 - val_accuracy: 0.9521 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9858 - val_f1score: 0.0000e+00\n",
      "Epoch 15/200\n",
      "92/92 [==============================] - 84s 914ms/step - loss: 307.1634 - accuracy: 0.9887 - precision: 0.9844 - recall: 0.9932 - auc: 0.9991 - f1score: 0.9888 - val_loss: 321.3893 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9825 - val_f1score: 0.0000e+00\n",
      "Epoch 16/200\n",
      "92/92 [==============================] - 84s 914ms/step - loss: 297.9525 - accuracy: 0.9887 - precision: 0.9890 - recall: 0.9884 - auc: 0.9982 - f1score: 0.9887 - val_loss: 312.0521 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9843 - val_f1score: 0.0000e+00\n",
      "Epoch 17/200\n",
      "92/92 [==============================] - 84s 914ms/step - loss: 289.3331 - accuracy: 0.9949 - precision: 0.9939 - recall: 0.9959 - auc: 0.9994 - f1score: 0.9949 - val_loss: 303.3612 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9779 - val_f1score: 0.0000e+00\n",
      "Epoch 18/200\n",
      "92/92 [==============================] - 84s 911ms/step - loss: 281.2524 - accuracy: 0.9904 - precision: 0.9871 - recall: 0.9938 - auc: 0.9991 - f1score: 0.9905 - val_loss: 295.5222 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9785 - val_f1score: 0.0000e+00\n",
      "Epoch 19/200\n",
      "92/92 [==============================] - 84s 914ms/step - loss: 273.4492 - accuracy: 0.9908 - precision: 0.9898 - recall: 0.9918 - auc: 0.9994 - f1score: 0.9908 - val_loss: 288.1125 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9749 - val_f1score: 0.0000e+00\n",
      "Epoch 20/200\n",
      "92/92 [==============================] - 84s 915ms/step - loss: 266.3653 - accuracy: 0.9925 - precision: 0.9905 - recall: 0.9945 - auc: 0.9986 - f1score: 0.9925 - val_loss: 280.9428 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9778 - val_f1score: 0.0000e+00\n",
      "Epoch 21/200\n",
      "92/92 [==============================] - 84s 915ms/step - loss: 259.4434 - accuracy: 0.9925 - precision: 0.9911 - recall: 0.9938 - auc: 0.9993 - f1score: 0.9925 - val_loss: 274.4083 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 22/200\n",
      "92/92 [==============================] - 84s 915ms/step - loss: 252.8643 - accuracy: 0.9932 - precision: 0.9925 - recall: 0.9938 - auc: 0.9997 - f1score: 0.9932 - val_loss: 268.1873 - val_accuracy: 0.9247 - val_precision: 0.1000 - val_recall: 0.3333 - val_auc: 0.9768 - val_f1score: 0.1538\n",
      "Epoch 23/200\n",
      "92/92 [==============================] - 86s 939ms/step - loss: 246.7632 - accuracy: 0.9942 - precision: 0.9932 - recall: 0.9952 - auc: 0.9998 - f1score: 0.9942 - val_loss: 261.9671 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9782 - val_f1score: 0.0000e+00\n",
      "Epoch 24/200\n",
      "92/92 [==============================] - 84s 913ms/step - loss: 240.7310 - accuracy: 0.9914 - precision: 0.9918 - recall: 0.9911 - auc: 0.9994 - f1score: 0.9914 - val_loss: 256.6082 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9792 - val_f1score: 0.0000e+00\n",
      "Epoch 25/200\n",
      "92/92 [==============================] - 85s 925ms/step - loss: 235.1318 - accuracy: 0.9856 - precision: 0.9830 - recall: 0.9884 - auc: 0.9973 - f1score: 0.9857 - val_loss: 250.8067 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9785 - val_f1score: 0.0000e+00\n",
      "Epoch 26/200\n",
      "92/92 [==============================] - 86s 931ms/step - loss: 229.6448 - accuracy: 0.9925 - precision: 0.9911 - recall: 0.9938 - auc: 0.9997 - f1score: 0.9925 - val_loss: 245.4219 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9835 - val_f1score: 0.0000e+00\n",
      "Epoch 27/200\n",
      "92/92 [==============================] - 88s 956ms/step - loss: 224.4741 - accuracy: 0.9966 - precision: 0.9959 - recall: 0.9973 - auc: 0.9999 - f1score: 0.9966 - val_loss: 240.2623 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9834 - val_f1score: 0.0000e+00\n",
      "Epoch 28/200\n",
      "92/92 [==============================] - 86s 937ms/step - loss: 219.3220 - accuracy: 0.9949 - precision: 0.9939 - recall: 0.9959 - auc: 0.9999 - f1score: 0.9949 - val_loss: 235.4506 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 29/200\n",
      "92/92 [==============================] - 86s 932ms/step - loss: 214.4769 - accuracy: 0.9952 - precision: 0.9952 - recall: 0.9952 - auc: 0.9995 - f1score: 0.9952 - val_loss: 230.5605 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9878 - val_f1score: 0.0000e+00\n",
      "Epoch 30/200\n",
      "92/92 [==============================] - 84s 916ms/step - loss: 209.8143 - accuracy: 0.9935 - precision: 0.9932 - recall: 0.9938 - auc: 0.9998 - f1score: 0.9935 - val_loss: 226.4063 - val_accuracy: 0.8767 - val_precision: 0.0588 - val_recall: 0.3333 - val_auc: 0.9421 - val_f1score: 0.1000\n",
      "Epoch 31/200\n",
      "92/92 [==============================] - 84s 913ms/step - loss: 205.2325 - accuracy: 0.9860 - precision: 0.9850 - recall: 0.9870 - auc: 0.9979 - f1score: 0.9860 - val_loss: 222.2711 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9799 - val_f1score: 0.0000e+00\n",
      "Epoch 32/200\n",
      "92/92 [==============================] - 84s 913ms/step - loss: 200.8640 - accuracy: 0.9956 - precision: 0.9945 - recall: 0.9966 - auc: 0.9995 - f1score: 0.9956 - val_loss: 218.0698 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9789 - val_f1score: 0.0000e+00\n",
      "Epoch 33/200\n",
      "92/92 [==============================] - 84s 910ms/step - loss: 196.6483 - accuracy: 0.9808 - precision: 0.9815 - recall: 0.9802 - auc: 0.9961 - f1score: 0.9808 - val_loss: 213.7967 - val_accuracy: 0.9110 - val_precision: 0.1429 - val_recall: 0.6667 - val_auc: 0.9510 - val_f1score: 0.2353\n",
      "Epoch 34/200\n",
      "92/92 [==============================] - 85s 925ms/step - loss: 192.5894 - accuracy: 0.9918 - precision: 0.9891 - recall: 0.9945 - auc: 0.9998 - f1score: 0.9918 - val_loss: 209.8233 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9773 - val_f1score: 0.0000e+00\n",
      "Epoch 35/200\n",
      "92/92 [==============================] - 85s 926ms/step - loss: 188.5200 - accuracy: 0.9819 - precision: 0.9796 - recall: 0.9843 - auc: 0.9978 - f1score: 0.9819 - val_loss: 206.0394 - val_accuracy: 0.8836 - val_precision: 0.0625 - val_recall: 0.3333 - val_auc: 0.9515 - val_f1score: 0.1053\n",
      "Epoch 36/200\n",
      "92/92 [==============================] - 84s 915ms/step - loss: 184.5976 - accuracy: 0.9822 - precision: 0.9796 - recall: 0.9849 - auc: 0.9970 - f1score: 0.9823 - val_loss: 202.0704 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9785 - val_f1score: 0.0000e+00\n",
      "Epoch 37/200\n",
      "92/92 [==============================] - 85s 920ms/step - loss: 180.7956 - accuracy: 0.9973 - precision: 0.9973 - recall: 0.9973 - auc: 1.0000 - f1score: 0.9973 - val_loss: 198.1953 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9778 - val_f1score: 0.0000e+00\n",
      "Epoch 38/200\n",
      "92/92 [==============================] - 85s 926ms/step - loss: 176.9435 - accuracy: 0.9969 - precision: 0.9959 - recall: 0.9979 - auc: 1.0000 - f1score: 0.9969 - val_loss: 194.4908 - val_accuracy: 0.9589 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9824 - val_f1score: 0.0000e+00\n",
      "Epoch 39/200\n",
      "92/92 [==============================] - 84s 914ms/step - loss: 173.3947 - accuracy: 0.9969 - precision: 0.9973 - recall: 0.9966 - auc: 0.9999 - f1score: 0.9969 - val_loss: 190.8368 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9779 - val_f1score: 0.0000e+00\n",
      "Epoch 40/200\n",
      "92/92 [==============================] - 84s 910ms/step - loss: 169.8620 - accuracy: 0.9778 - precision: 0.9768 - recall: 0.9788 - auc: 0.9981 - f1score: 0.9778 - val_loss: 187.7195 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 41/200\n",
      "92/92 [==============================] - 83s 902ms/step - loss: 166.3966 - accuracy: 0.9918 - precision: 0.9911 - recall: 0.9925 - auc: 0.9995 - f1score: 0.9918 - val_loss: 183.8917 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9773 - val_f1score: 0.0000e+00\n",
      "Epoch 42/200\n",
      "92/92 [==============================] - 82s 896ms/step - loss: 163.0598 - accuracy: 0.9921 - precision: 0.9918 - recall: 0.9925 - auc: 0.9987 - f1score: 0.9921 - val_loss: 180.5403 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9783 - val_f1score: 0.0000e+00\n",
      "Epoch 43/200\n",
      "92/92 [==============================] - 83s 899ms/step - loss: 159.6289 - accuracy: 0.9959 - precision: 0.9945 - recall: 0.9973 - auc: 0.9999 - f1score: 0.9959 - val_loss: 177.3083 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9787 - val_f1score: 0.0000e+00\n",
      "Epoch 44/200\n",
      "92/92 [==============================] - 84s 908ms/step - loss: 156.4277 - accuracy: 0.9942 - precision: 0.9945 - recall: 0.9938 - auc: 0.9995 - f1score: 0.9942 - val_loss: 173.6967 - val_accuracy: 0.9521 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9812 - val_f1score: 0.0000e+00\n",
      "Epoch 45/200\n",
      "92/92 [==============================] - 82s 895ms/step - loss: 153.1181 - accuracy: 0.9956 - precision: 0.9952 - recall: 0.9959 - auc: 0.9998 - f1score: 0.9956 - val_loss: 171.0964 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 46/200\n",
      "92/92 [==============================] - 83s 899ms/step - loss: 150.1625 - accuracy: 0.9877 - precision: 0.9863 - recall: 0.9890 - auc: 0.9984 - f1score: 0.9877 - val_loss: 167.5058 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9753 - val_f1score: 0.0000e+00\n",
      "Epoch 47/200\n",
      "92/92 [==============================] - 83s 903ms/step - loss: 147.0145 - accuracy: 0.9935 - precision: 0.9918 - recall: 0.9952 - auc: 0.9987 - f1score: 0.9935 - val_loss: 164.7827 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 48/200\n",
      "92/92 [==============================] - 83s 904ms/step - loss: 143.9577 - accuracy: 0.9911 - precision: 0.9904 - recall: 0.9918 - auc: 0.9990 - f1score: 0.9911 - val_loss: 161.5535 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9750 - val_f1score: 0.0000e+00\n",
      "Epoch 49/200\n",
      "92/92 [==============================] - 83s 899ms/step - loss: 141.0168 - accuracy: 0.9914 - precision: 0.9911 - recall: 0.9918 - auc: 0.9994 - f1score: 0.9914 - val_loss: 158.9459 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 50/200\n",
      "92/92 [==============================] - 82s 896ms/step - loss: 138.1187 - accuracy: 0.9784 - precision: 0.9768 - recall: 0.9802 - auc: 0.9967 - f1score: 0.9785 - val_loss: 155.7944 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 51/200\n",
      "92/92 [==============================] - 82s 896ms/step - loss: 135.2272 - accuracy: 0.9942 - precision: 0.9918 - recall: 0.9966 - auc: 0.9991 - f1score: 0.9942 - val_loss: 152.9199 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9787 - val_f1score: 0.0000e+00\n",
      "Epoch 52/200\n",
      "92/92 [==============================] - 83s 901ms/step - loss: 132.4074 - accuracy: 0.9935 - precision: 0.9925 - recall: 0.9945 - auc: 0.9995 - f1score: 0.9935 - val_loss: 150.0083 - val_accuracy: 0.9521 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9735 - val_f1score: 0.0000e+00\n",
      "Epoch 53/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 82s 894ms/step - loss: 129.7171 - accuracy: 0.9979 - precision: 0.9973 - recall: 0.9986 - auc: 1.0000 - f1score: 0.9979 - val_loss: 147.0752 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9779 - val_f1score: 0.0000e+00\n",
      "Epoch 54/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 126.9680 - accuracy: 0.9959 - precision: 0.9959 - recall: 0.9959 - auc: 0.9999 - f1score: 0.9959 - val_loss: 144.5956 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9783 - val_f1score: 0.0000e+00\n",
      "Epoch 55/200\n",
      "92/92 [==============================] - 83s 898ms/step - loss: 124.3792 - accuracy: 0.9870 - precision: 0.9870 - recall: 0.9870 - auc: 0.9987 - f1score: 0.9870 - val_loss: 141.8273 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9839 - val_f1score: 0.0000e+00\n",
      "Epoch 56/200\n",
      "92/92 [==============================] - 82s 895ms/step - loss: 121.7867 - accuracy: 0.9952 - precision: 0.9952 - recall: 0.9952 - auc: 0.9992 - f1score: 0.9952 - val_loss: 139.2465 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9780 - val_f1score: 0.0000e+00\n",
      "Epoch 57/200\n",
      "92/92 [==============================] - 82s 893ms/step - loss: 119.2542 - accuracy: 0.9952 - precision: 0.9945 - recall: 0.9959 - auc: 0.9999 - f1score: 0.9952 - val_loss: 136.4818 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 58/200\n",
      "92/92 [==============================] - 82s 895ms/step - loss: 116.7384 - accuracy: 0.9904 - precision: 0.9877 - recall: 0.9932 - auc: 0.9985 - f1score: 0.9904 - val_loss: 133.8117 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9772 - val_f1score: 0.0000e+00\n",
      "Epoch 59/200\n",
      "92/92 [==============================] - 82s 896ms/step - loss: 114.3749 - accuracy: 0.9665 - precision: 0.9646 - recall: 0.9685 - auc: 0.9938 - f1score: 0.9665 - val_loss: 131.5633 - val_accuracy: 0.9110 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9499 - val_f1score: 0.0000e+00\n",
      "Epoch 60/200\n",
      "92/92 [==============================] - 83s 897ms/step - loss: 112.0249 - accuracy: 0.9760 - precision: 0.9747 - recall: 0.9774 - auc: 0.9964 - f1score: 0.9761 - val_loss: 129.2455 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9782 - val_f1score: 0.0000e+00\n",
      "Epoch 61/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 109.6026 - accuracy: 0.9973 - precision: 0.9959 - recall: 0.9986 - auc: 0.9995 - f1score: 0.9973 - val_loss: 127.0432 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9792 - val_f1score: 0.0000e+00\n",
      "Epoch 62/200\n",
      "92/92 [==============================] - 83s 899ms/step - loss: 107.3394 - accuracy: 0.9956 - precision: 0.9959 - recall: 0.9952 - auc: 0.9999 - f1score: 0.9955 - val_loss: 124.3352 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9776 - val_f1score: 0.0000e+00\n",
      "Epoch 63/200\n",
      "92/92 [==============================] - 82s 895ms/step - loss: 105.1306 - accuracy: 0.9949 - precision: 0.9945 - recall: 0.9952 - auc: 0.9999 - f1score: 0.9949 - val_loss: 122.0982 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9785 - val_f1score: 0.0000e+00\n",
      "Epoch 64/200\n",
      "92/92 [==============================] - 82s 896ms/step - loss: 102.9359 - accuracy: 0.9863 - precision: 0.9837 - recall: 0.9890 - auc: 0.9982 - f1score: 0.9863 - val_loss: 120.0171 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 65/200\n",
      "92/92 [==============================] - 82s 896ms/step - loss: 100.6932 - accuracy: 0.9956 - precision: 0.9952 - recall: 0.9959 - auc: 0.9992 - f1score: 0.9956 - val_loss: 117.6513 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9775 - val_f1score: 0.0000e+00\n",
      "Epoch 66/200\n",
      "92/92 [==============================] - 82s 895ms/step - loss: 98.5703 - accuracy: 0.9952 - precision: 0.9945 - recall: 0.9959 - auc: 0.9999 - f1score: 0.9952 - val_loss: 115.2852 - val_accuracy: 0.9521 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9672 - val_f1score: 0.0000e+00\n",
      "Epoch 67/200\n",
      "92/92 [==============================] - 82s 894ms/step - loss: 96.5628 - accuracy: 0.9908 - precision: 0.9918 - recall: 0.9897 - auc: 0.9984 - f1score: 0.9908 - val_loss: 113.0714 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9762 - val_f1score: 0.0000e+00\n",
      "Epoch 68/200\n",
      "92/92 [==============================] - 82s 892ms/step - loss: 94.5559 - accuracy: 0.9846 - precision: 0.9836 - recall: 0.9856 - auc: 0.9980 - f1score: 0.9846 - val_loss: 111.3699 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 69/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 92.5778 - accuracy: 0.9935 - precision: 0.9932 - recall: 0.9938 - auc: 0.9991 - f1score: 0.9935 - val_loss: 109.2199 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 70/200\n",
      "92/92 [==============================] - 82s 895ms/step - loss: 90.5991 - accuracy: 0.9938 - precision: 0.9938 - recall: 0.9938 - auc: 0.9988 - f1score: 0.9938 - val_loss: 106.9774 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9781 - val_f1score: 0.0000e+00\n",
      "Epoch 71/200\n",
      "92/92 [==============================] - 82s 893ms/step - loss: 88.7127 - accuracy: 0.9928 - precision: 0.9911 - recall: 0.9945 - auc: 0.9981 - f1score: 0.9928 - val_loss: 105.1384 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 72/200\n",
      "92/92 [==============================] - 82s 894ms/step - loss: 86.8151 - accuracy: 0.9894 - precision: 0.9897 - recall: 0.9890 - auc: 0.9991 - f1score: 0.9894 - val_loss: 103.1046 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9778 - val_f1score: 0.0000e+00\n",
      "Epoch 73/200\n",
      "92/92 [==============================] - 82s 892ms/step - loss: 84.9959 - accuracy: 0.9949 - precision: 0.9939 - recall: 0.9959 - auc: 0.9998 - f1score: 0.9949 - val_loss: 101.0543 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9786 - val_f1score: 0.0000e+00\n",
      "Epoch 74/200\n",
      "92/92 [==============================] - 82s 891ms/step - loss: 83.1342 - accuracy: 0.9928 - precision: 0.9938 - recall: 0.9918 - auc: 0.9998 - f1score: 0.9928 - val_loss: 98.7221 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9773 - val_f1score: 0.0000e+00\n",
      "Epoch 75/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 81.4094 - accuracy: 0.9945 - precision: 0.9938 - recall: 0.9952 - auc: 0.9999 - f1score: 0.9945 - val_loss: 97.1308 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9786 - val_f1score: 0.0000e+00\n",
      "Epoch 76/200\n",
      "92/92 [==============================] - 82s 892ms/step - loss: 79.6973 - accuracy: 0.9949 - precision: 0.9945 - recall: 0.9952 - auc: 0.9992 - f1score: 0.9949 - val_loss: 95.4762 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 77/200\n",
      "92/92 [==============================] - 82s 891ms/step - loss: 78.1196 - accuracy: 0.9699 - precision: 0.9686 - recall: 0.9713 - auc: 0.9929 - f1score: 0.9699 - val_loss: 93.5991 - val_accuracy: 0.8973 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9422 - val_f1score: 0.0000e+00\n",
      "Epoch 78/200\n",
      "92/92 [==============================] - 82s 896ms/step - loss: 76.3911 - accuracy: 0.9935 - precision: 0.9938 - recall: 0.9932 - auc: 0.9994 - f1score: 0.9935 - val_loss: 92.0594 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9771 - val_f1score: 0.0000e+00\n",
      "Epoch 79/200\n",
      "92/92 [==============================] - 82s 891ms/step - loss: 74.8704 - accuracy: 0.9856 - precision: 0.9850 - recall: 0.9863 - auc: 0.9983 - f1score: 0.9856 - val_loss: 90.2284 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 80/200\n",
      "92/92 [==============================] - 82s 888ms/step - loss: 73.3737 - accuracy: 0.9579 - precision: 0.9563 - recall: 0.9596 - auc: 0.9902 - f1score: 0.9580 - val_loss: 88.6911 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9778 - val_f1score: 0.0000e+00\n",
      "Epoch 81/200\n",
      "92/92 [==============================] - 81s 885ms/step - loss: 71.7835 - accuracy: 0.9945 - precision: 0.9945 - recall: 0.9945 - auc: 0.9992 - f1score: 0.9945 - val_loss: 87.3624 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9787 - val_f1score: 0.0000e+00\n",
      "Epoch 82/200\n",
      "92/92 [==============================] - 81s 886ms/step - loss: 70.3414 - accuracy: 0.9956 - precision: 0.9945 - recall: 0.9966 - auc: 0.9996 - f1score: 0.9956 - val_loss: 85.5297 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9772 - val_f1score: 0.0000e+00\n",
      "Epoch 83/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 68.9282 - accuracy: 0.9918 - precision: 0.9904 - recall: 0.9932 - auc: 0.9984 - f1score: 0.9918 - val_loss: 83.7460 - val_accuracy: 0.9452 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9656 - val_f1score: 0.0000e+00\n",
      "Epoch 84/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 67.4929 - accuracy: 0.9870 - precision: 0.9877 - recall: 0.9863 - auc: 0.9988 - f1score: 0.9870 - val_loss: 82.6905 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 85/200\n",
      "92/92 [==============================] - 81s 879ms/step - loss: 66.1403 - accuracy: 0.9890 - precision: 0.9877 - recall: 0.9904 - auc: 0.9984 - f1score: 0.9891 - val_loss: 81.6592 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 86/200\n",
      "92/92 [==============================] - 81s 881ms/step - loss: 64.7692 - accuracy: 0.9904 - precision: 0.9897 - recall: 0.9911 - auc: 0.9983 - f1score: 0.9904 - val_loss: 79.7207 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 87/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 63.4702 - accuracy: 0.9897 - precision: 0.9884 - recall: 0.9911 - auc: 0.9981 - f1score: 0.9897 - val_loss: 78.3416 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9789 - val_f1score: 0.0000e+00\n",
      "Epoch 88/200\n",
      "92/92 [==============================] - 82s 889ms/step - loss: 62.1288 - accuracy: 0.9932 - precision: 0.9932 - recall: 0.9932 - auc: 0.9995 - f1score: 0.9932 - val_loss: 77.0647 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 89/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 60.9486 - accuracy: 0.9973 - precision: 0.9959 - recall: 0.9986 - auc: 0.9996 - f1score: 0.9973 - val_loss: 75.3868 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9789 - val_f1score: 0.0000e+00\n",
      "Epoch 90/200\n",
      "92/92 [==============================] - 81s 883ms/step - loss: 59.7129 - accuracy: 0.9914 - precision: 0.9904 - recall: 0.9925 - auc: 0.9990 - f1score: 0.9915 - val_loss: 74.1957 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 91/200\n",
      "92/92 [==============================] - 82s 889ms/step - loss: 58.4745 - accuracy: 0.9949 - precision: 0.9945 - recall: 0.9952 - auc: 0.9999 - f1score: 0.9949 - val_loss: 72.6952 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9779 - val_f1score: 0.0000e+00\n",
      "Epoch 92/200\n",
      "92/92 [==============================] - 81s 882ms/step - loss: 57.3878 - accuracy: 0.9863 - precision: 0.9837 - recall: 0.9890 - auc: 0.9975 - f1score: 0.9863 - val_loss: 71.2366 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9765 - val_f1score: 0.0000e+00\n",
      "Epoch 93/200\n",
      "92/92 [==============================] - 82s 888ms/step - loss: 56.2475 - accuracy: 0.9880 - precision: 0.9884 - recall: 0.9877 - auc: 0.9995 - f1score: 0.9880 - val_loss: 70.0738 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9778 - val_f1score: 0.0000e+00\n",
      "Epoch 94/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 55.0579 - accuracy: 0.9973 - precision: 0.9966 - recall: 0.9979 - auc: 1.0000 - f1score: 0.9973 - val_loss: 68.8727 - val_accuracy: 0.9521 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9752 - val_f1score: 0.0000e+00\n",
      "Epoch 95/200\n",
      "92/92 [==============================] - 82s 889ms/step - loss: 54.1224 - accuracy: 0.9849 - precision: 0.9830 - recall: 0.9870 - auc: 0.9983 - f1score: 0.9850 - val_loss: 67.4122 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9783 - val_f1score: 0.0000e+00\n",
      "Epoch 96/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 52.9569 - accuracy: 0.9918 - precision: 0.9918 - recall: 0.9918 - auc: 0.9997 - f1score: 0.9918 - val_loss: 66.8182 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 97/200\n",
      "92/92 [==============================] - 82s 892ms/step - loss: 52.0628 - accuracy: 0.9808 - precision: 0.9808 - recall: 0.9808 - auc: 0.9960 - f1score: 0.9808 - val_loss: 65.3919 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9782 - val_f1score: 0.0000e+00\n",
      "Epoch 98/200\n",
      "92/92 [==============================] - 82s 892ms/step - loss: 51.0448 - accuracy: 0.9802 - precision: 0.9788 - recall: 0.9815 - auc: 0.9954 - f1score: 0.9802 - val_loss: 66.1252 - val_accuracy: 0.5959 - val_precision: 0.0333 - val_recall: 0.6667 - val_auc: 0.6291 - val_f1score: 0.0635\n",
      "Epoch 99/200\n",
      "92/92 [==============================] - 81s 885ms/step - loss: 50.1200 - accuracy: 0.9877 - precision: 0.9857 - recall: 0.9897 - auc: 0.9978 - f1score: 0.9877 - val_loss: 63.2803 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9787 - val_f1score: 0.0000e+00\n",
      "Epoch 100/200\n",
      "92/92 [==============================] - 82s 888ms/step - loss: 49.1249 - accuracy: 0.9921 - precision: 0.9911 - recall: 0.9932 - auc: 0.9991 - f1score: 0.9921 - val_loss: 62.3913 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 101/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 48.2218 - accuracy: 0.9935 - precision: 0.9945 - recall: 0.9925 - auc: 0.9995 - f1score: 0.9935 - val_loss: 61.4509 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9787 - val_f1score: 0.0000e+00\n",
      "Epoch 102/200\n",
      "92/92 [==============================] - 82s 888ms/step - loss: 47.3797 - accuracy: 0.9949 - precision: 0.9932 - recall: 0.9966 - auc: 0.9992 - f1score: 0.9949 - val_loss: 60.5121 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9792 - val_f1score: 0.0000e+00\n",
      "Epoch 103/200\n",
      "92/92 [==============================] - 81s 885ms/step - loss: 46.5730 - accuracy: 0.9863 - precision: 0.9870 - recall: 0.9856 - auc: 0.9969 - f1score: 0.9863 - val_loss: 59.3436 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9758 - val_f1score: 0.0000e+00\n",
      "Epoch 104/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 45.6332 - accuracy: 0.9962 - precision: 0.9952 - recall: 0.9973 - auc: 0.9999 - f1score: 0.9962 - val_loss: 58.5792 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 81s 885ms/step - loss: 44.8816 - accuracy: 0.9983 - precision: 0.9979 - recall: 0.9986 - auc: 1.0000 - f1score: 0.9983 - val_loss: 57.5167 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 106/200\n",
      "92/92 [==============================] - 81s 883ms/step - loss: 44.1171 - accuracy: 0.9925 - precision: 0.9905 - recall: 0.9945 - auc: 0.9984 - f1score: 0.9925 - val_loss: 56.5009 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9772 - val_f1score: 0.0000e+00\n",
      "Epoch 107/200\n",
      "92/92 [==============================] - 81s 884ms/step - loss: 43.3631 - accuracy: 0.9832 - precision: 0.9836 - recall: 0.9829 - auc: 0.9952 - f1score: 0.9832 - val_loss: 56.1622 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 108/200\n",
      "92/92 [==============================] - 81s 883ms/step - loss: 42.6489 - accuracy: 0.9805 - precision: 0.9808 - recall: 0.9802 - auc: 0.9970 - f1score: 0.9805 - val_loss: 55.2447 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9783 - val_f1score: 0.0000e+00\n",
      "Epoch 109/200\n",
      "92/92 [==============================] - 81s 884ms/step - loss: 41.9462 - accuracy: 0.9822 - precision: 0.9822 - recall: 0.9822 - auc: 0.9980 - f1score: 0.9822 - val_loss: 54.1658 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9782 - val_f1score: 0.0000e+00\n",
      "Epoch 110/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 41.2073 - accuracy: 0.9952 - precision: 0.9945 - recall: 0.9959 - auc: 0.9995 - f1score: 0.9952 - val_loss: 53.3313 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 111/200\n",
      "92/92 [==============================] - 81s 885ms/step - loss: 40.5684 - accuracy: 0.9962 - precision: 0.9952 - recall: 0.9973 - auc: 0.9999 - f1score: 0.9962 - val_loss: 52.5793 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 112/200\n",
      "92/92 [==============================] - 81s 884ms/step - loss: 39.9240 - accuracy: 0.9966 - precision: 0.9952 - recall: 0.9979 - auc: 0.9996 - f1score: 0.9966 - val_loss: 51.8026 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 113/200\n",
      "92/92 [==============================] - 82s 892ms/step - loss: 39.2642 - accuracy: 0.9959 - precision: 0.9952 - recall: 0.9966 - auc: 0.9995 - f1score: 0.9959 - val_loss: 51.1446 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9771 - val_f1score: 0.0000e+00\n",
      "Epoch 114/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 38.6529 - accuracy: 0.9932 - precision: 0.9918 - recall: 0.9945 - auc: 0.9990 - f1score: 0.9932 - val_loss: 50.6917 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 115/200\n",
      "92/92 [==============================] - 81s 884ms/step - loss: 38.0059 - accuracy: 0.9945 - precision: 0.9932 - recall: 0.9959 - auc: 0.9999 - f1score: 0.9945 - val_loss: 49.7463 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9772 - val_f1score: 0.0000e+00\n",
      "Epoch 116/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 37.5400 - accuracy: 0.9832 - precision: 0.9836 - recall: 0.9829 - auc: 0.9967 - f1score: 0.9832 - val_loss: 48.8056 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9780 - val_f1score: 0.0000e+00\n",
      "Epoch 117/200\n",
      "92/92 [==============================] - 81s 884ms/step - loss: 36.9257 - accuracy: 0.9904 - precision: 0.9884 - recall: 0.9925 - auc: 0.9993 - f1score: 0.9904 - val_loss: 48.7223 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 118/200\n",
      "92/92 [==============================] - 81s 885ms/step - loss: 36.3700 - accuracy: 0.9890 - precision: 0.9884 - recall: 0.9897 - auc: 0.9993 - f1score: 0.9891 - val_loss: 47.8207 - val_accuracy: 0.9384 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9776 - val_f1score: 0.0000e+00\n",
      "Epoch 119/200\n",
      "92/92 [==============================] - 82s 886ms/step - loss: 35.8817 - accuracy: 0.9897 - precision: 0.9891 - recall: 0.9904 - auc: 0.9985 - f1score: 0.9897 - val_loss: 47.0120 - val_accuracy: 0.9178 - val_precision: 0.0909 - val_recall: 0.3333 - val_auc: 0.9587 - val_f1score: 0.1429\n",
      "Epoch 120/200\n",
      "92/92 [==============================] - 82s 886ms/step - loss: 35.2675 - accuracy: 0.9959 - precision: 0.9959 - recall: 0.9959 - auc: 0.9995 - f1score: 0.9959 - val_loss: 46.6828 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9789 - val_f1score: 0.0000e+00\n",
      "Epoch 121/200\n",
      "92/92 [==============================] - 82s 886ms/step - loss: 34.7940 - accuracy: 0.9959 - precision: 0.9959 - recall: 0.9959 - auc: 0.9989 - f1score: 0.9959 - val_loss: 46.3212 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 122/200\n",
      "92/92 [==============================] - 81s 885ms/step - loss: 34.3026 - accuracy: 0.9969 - precision: 0.9966 - recall: 0.9973 - auc: 0.9993 - f1score: 0.9969 - val_loss: 45.6273 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 123/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 33.7958 - accuracy: 0.9945 - precision: 0.9938 - recall: 0.9952 - auc: 0.9988 - f1score: 0.9945 - val_loss: 45.2522 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 124/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 33.3592 - accuracy: 0.9956 - precision: 0.9945 - recall: 0.9966 - auc: 0.9995 - f1score: 0.9956 - val_loss: 44.5863 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 125/200\n",
      "92/92 [==============================] - 81s 885ms/step - loss: 32.8952 - accuracy: 0.9966 - precision: 0.9973 - recall: 0.9959 - auc: 0.9999 - f1score: 0.9966 - val_loss: 43.9452 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9775 - val_f1score: 0.0000e+00\n",
      "Epoch 126/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 32.4567 - accuracy: 0.9894 - precision: 0.9884 - recall: 0.9904 - auc: 0.9989 - f1score: 0.9894 - val_loss: 43.2964 - val_accuracy: 0.9589 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9764 - val_f1score: 0.0000e+00\n",
      "Epoch 127/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 32.0156 - accuracy: 0.9935 - precision: 0.9932 - recall: 0.9938 - auc: 0.9992 - f1score: 0.9935 - val_loss: 42.9536 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9779 - val_f1score: 0.0000e+00\n",
      "Epoch 128/200\n",
      "92/92 [==============================] - 82s 889ms/step - loss: 31.6369 - accuracy: 0.9918 - precision: 0.9911 - recall: 0.9925 - auc: 0.9995 - f1score: 0.9918 - val_loss: 42.6316 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9792 - val_f1score: 0.0000e+00\n",
      "Epoch 129/200\n",
      "92/92 [==============================] - 82s 893ms/step - loss: 31.2701 - accuracy: 0.9825 - precision: 0.9842 - recall: 0.9808 - auc: 0.9965 - f1score: 0.9825 - val_loss: 42.2846 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 130/200\n",
      "92/92 [==============================] - 81s 883ms/step - loss: 30.8372 - accuracy: 0.9914 - precision: 0.9904 - recall: 0.9925 - auc: 0.9984 - f1score: 0.9915 - val_loss: 41.5220 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9771 - val_f1score: 0.0000e+00\n",
      "Epoch 131/200\n",
      "92/92 [==============================] - 82s 888ms/step - loss: 30.4552 - accuracy: 0.9966 - precision: 0.9952 - recall: 0.9979 - auc: 0.9999 - f1score: 0.9966 - val_loss: 40.9916 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 132/200\n",
      "92/92 [==============================] - 82s 888ms/step - loss: 30.0638 - accuracy: 0.9935 - precision: 0.9945 - recall: 0.9925 - auc: 0.9991 - f1score: 0.9935 - val_loss: 40.6398 - val_accuracy: 0.9521 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9760 - val_f1score: 0.0000e+00\n",
      "Epoch 133/200\n",
      "92/92 [==============================] - 81s 884ms/step - loss: 29.7520 - accuracy: 0.9935 - precision: 0.9911 - recall: 0.9959 - auc: 0.9987 - f1score: 0.9935 - val_loss: 40.0747 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9786 - val_f1score: 0.0000e+00\n",
      "Epoch 134/200\n",
      "92/92 [==============================] - 81s 884ms/step - loss: 29.4282 - accuracy: 0.9890 - precision: 0.9877 - recall: 0.9904 - auc: 0.9971 - f1score: 0.9891 - val_loss: 39.6636 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 135/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 29.1048 - accuracy: 0.9853 - precision: 0.9830 - recall: 0.9877 - auc: 0.9975 - f1score: 0.9853 - val_loss: 39.2798 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9784 - val_f1score: 0.0000e+00\n",
      "Epoch 136/200\n",
      "92/92 [==============================] - 81s 882ms/step - loss: 28.7529 - accuracy: 0.9942 - precision: 0.9938 - recall: 0.9945 - auc: 0.9992 - f1score: 0.9942 - val_loss: 38.9132 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9788 - val_f1score: 0.0000e+00\n",
      "Epoch 137/200\n",
      "92/92 [==============================] - 81s 885ms/step - loss: 28.4341 - accuracy: 0.9932 - precision: 0.9925 - recall: 0.9938 - auc: 0.9994 - f1score: 0.9932 - val_loss: 38.4931 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9773 - val_f1score: 0.0000e+00\n",
      "Epoch 138/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 28.1246 - accuracy: 0.9966 - precision: 0.9959 - recall: 0.9973 - auc: 0.9989 - f1score: 0.9966 - val_loss: 38.3020 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9789 - val_f1score: 0.0000e+00\n",
      "Epoch 139/200\n",
      "92/92 [==============================] - 82s 893ms/step - loss: 27.8415 - accuracy: 0.9901 - precision: 0.9897 - recall: 0.9904 - auc: 0.9988 - f1score: 0.9901 - val_loss: 38.1610 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9792 - val_f1score: 0.0000e+00\n",
      "Epoch 140/200\n",
      "92/92 [==============================] - 85s 927ms/step - loss: 27.5841 - accuracy: 0.9846 - precision: 0.9836 - recall: 0.9856 - auc: 0.9966 - f1score: 0.9846 - val_loss: 37.5907 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9788 - val_f1score: 0.0000e+00\n",
      "Epoch 141/200\n",
      "92/92 [==============================] - 84s 912ms/step - loss: 27.2464 - accuracy: 0.9911 - precision: 0.9911 - recall: 0.9911 - auc: 0.9976 - f1score: 0.9911 - val_loss: 37.2230 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9787 - val_f1score: 0.0000e+00\n",
      "Epoch 142/200\n",
      "92/92 [==============================] - 83s 899ms/step - loss: 26.9757 - accuracy: 0.9880 - precision: 0.9870 - recall: 0.9890 - auc: 0.9982 - f1score: 0.9880 - val_loss: 37.0055 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9788 - val_f1score: 0.0000e+00\n",
      "Epoch 143/200\n",
      "92/92 [==============================] - 83s 905ms/step - loss: 26.6756 - accuracy: 0.9973 - precision: 0.9966 - recall: 0.9979 - auc: 1.0000 - f1score: 0.9973 - val_loss: 36.6831 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 144/200\n",
      "92/92 [==============================] - 83s 906ms/step - loss: 26.4134 - accuracy: 0.9956 - precision: 0.9939 - recall: 0.9973 - auc: 0.9999 - f1score: 0.9956 - val_loss: 36.0167 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9713 - val_f1score: 0.0000e+00\n",
      "Epoch 145/200\n",
      "92/92 [==============================] - 83s 904ms/step - loss: 26.1602 - accuracy: 0.9904 - precision: 0.9897 - recall: 0.9911 - auc: 0.9983 - f1score: 0.9904 - val_loss: 35.8575 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 146/200\n",
      "92/92 [==============================] - 84s 909ms/step - loss: 25.9211 - accuracy: 0.9802 - precision: 0.9788 - recall: 0.9815 - auc: 0.9957 - f1score: 0.9802 - val_loss: 35.9923 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 147/200\n",
      "92/92 [==============================] - 83s 905ms/step - loss: 25.6647 - accuracy: 0.9894 - precision: 0.9917 - recall: 0.9870 - auc: 0.9975 - f1score: 0.9894 - val_loss: 35.1912 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9786 - val_f1score: 0.0000e+00\n",
      "Epoch 148/200\n",
      "92/92 [==============================] - 84s 915ms/step - loss: 25.4562 - accuracy: 0.9740 - precision: 0.9759 - recall: 0.9719 - auc: 0.9944 - f1score: 0.9739 - val_loss: 35.0102 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9842 - val_f1score: 0.0000e+00\n",
      "Epoch 149/200\n",
      "92/92 [==============================] - 84s 911ms/step - loss: 25.1423 - accuracy: 0.9914 - precision: 0.9898 - recall: 0.9932 - auc: 0.9994 - f1score: 0.9915 - val_loss: 34.8417 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9789 - val_f1score: 0.0000e+00\n",
      "Epoch 150/200\n",
      "92/92 [==============================] - 84s 910ms/step - loss: 24.9502 - accuracy: 0.9925 - precision: 0.9931 - recall: 0.9918 - auc: 0.9990 - f1score: 0.9925 - val_loss: 34.3673 - val_accuracy: 0.9589 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9771 - val_f1score: 0.0000e+00\n",
      "Epoch 151/200\n",
      "92/92 [==============================] - 84s 915ms/step - loss: 24.7267 - accuracy: 0.9959 - precision: 0.9959 - recall: 0.9959 - auc: 0.9996 - f1score: 0.9959 - val_loss: 34.0556 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9790 - val_f1score: 0.0000e+00\n",
      "Epoch 152/200\n",
      "92/92 [==============================] - 85s 919ms/step - loss: 24.4722 - accuracy: 0.9962 - precision: 0.9959 - recall: 0.9966 - auc: 0.9992 - f1score: 0.9962 - val_loss: 33.6083 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9848 - val_f1score: 0.0000e+00\n",
      "Epoch 153/200\n",
      "92/92 [==============================] - 84s 914ms/step - loss: 24.3249 - accuracy: 0.9836 - precision: 0.9869 - recall: 0.9802 - auc: 0.9970 - f1score: 0.9835 - val_loss: 33.6613 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 154/200\n",
      "92/92 [==============================] - 84s 915ms/step - loss: 24.0408 - accuracy: 0.9914 - precision: 0.9898 - recall: 0.9932 - auc: 0.9994 - f1score: 0.9915 - val_loss: 33.0506 - val_accuracy: 0.9589 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9837 - val_f1score: 0.0000e+00\n",
      "Epoch 155/200\n",
      "92/92 [==============================] - 83s 903ms/step - loss: 23.8214 - accuracy: 0.9935 - precision: 0.9945 - recall: 0.9925 - auc: 0.9991 - f1score: 0.9935 - val_loss: 33.0973 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 156/200\n",
      "92/92 [==============================] - 83s 900ms/step - loss: 23.5806 - accuracy: 0.9949 - precision: 0.9945 - recall: 0.9952 - auc: 0.9989 - f1score: 0.9949 - val_loss: 32.7073 - val_accuracy: 0.9452 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9672 - val_f1score: 0.0000e+00\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 82s 897ms/step - loss: 23.3898 - accuracy: 0.9969 - precision: 0.9959 - recall: 0.9979 - auc: 0.9996 - f1score: 0.9969 - val_loss: 32.4525 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9783 - val_f1score: 0.0000e+00\n",
      "Epoch 158/200\n",
      "92/92 [==============================] - 83s 898ms/step - loss: 23.1394 - accuracy: 0.9952 - precision: 0.9945 - recall: 0.9959 - auc: 0.9999 - f1score: 0.9952 - val_loss: 32.2933 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 159/200\n",
      "92/92 [==============================] - 82s 894ms/step - loss: 22.9985 - accuracy: 0.9880 - precision: 0.9870 - recall: 0.9890 - auc: 0.9986 - f1score: 0.9880 - val_loss: 31.6725 - val_accuracy: 0.9521 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9746 - val_f1score: 0.0000e+00\n",
      "Epoch 160/200\n",
      "92/92 [==============================] - 82s 893ms/step - loss: 22.7557 - accuracy: 0.9962 - precision: 0.9952 - recall: 0.9973 - auc: 0.9999 - f1score: 0.9962 - val_loss: 31.4604 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9784 - val_f1score: 0.0000e+00\n",
      "Epoch 161/200\n",
      "92/92 [==============================] - 82s 895ms/step - loss: 22.5777 - accuracy: 0.9973 - precision: 0.9973 - recall: 0.9973 - auc: 0.9999 - f1score: 0.9973 - val_loss: 31.0383 - val_accuracy: 0.9589 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9708 - val_f1score: 0.0000e+00\n",
      "Epoch 162/200\n",
      "92/92 [==============================] - 82s 893ms/step - loss: 22.3195 - accuracy: 0.9904 - precision: 0.9891 - recall: 0.9918 - auc: 0.9987 - f1score: 0.9904 - val_loss: 31.0437 - val_accuracy: 0.9315 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9651 - val_f1score: 0.0000e+00\n",
      "Epoch 163/200\n",
      "92/92 [==============================] - 82s 895ms/step - loss: 22.1508 - accuracy: 0.9914 - precision: 0.9904 - recall: 0.9925 - auc: 0.9987 - f1score: 0.9915 - val_loss: 30.9789 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9788 - val_f1score: 0.0000e+00\n",
      "Epoch 164/200\n",
      "92/92 [==============================] - 81s 885ms/step - loss: 21.9489 - accuracy: 0.9956 - precision: 0.9973 - recall: 0.9938 - auc: 0.9999 - f1score: 0.9955 - val_loss: 30.5195 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9722 - val_f1score: 0.0000e+00\n",
      "Epoch 165/200\n",
      "92/92 [==============================] - 82s 896ms/step - loss: 21.7570 - accuracy: 0.9956 - precision: 0.9952 - recall: 0.9959 - auc: 0.9999 - f1score: 0.9956 - val_loss: 30.1780 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9788 - val_f1score: 0.0000e+00\n",
      "Epoch 166/200\n",
      "92/92 [==============================] - 82s 893ms/step - loss: 21.5711 - accuracy: 0.9880 - precision: 0.9870 - recall: 0.9890 - auc: 0.9981 - f1score: 0.9880 - val_loss: 30.0352 - val_accuracy: 0.9178 - val_precision: 0.0909 - val_recall: 0.3333 - val_auc: 0.9513 - val_f1score: 0.1429\n",
      "Epoch 167/200\n",
      "92/92 [==============================] - 82s 895ms/step - loss: 21.3857 - accuracy: 0.9928 - precision: 0.9932 - recall: 0.9925 - auc: 0.9987 - f1score: 0.9928 - val_loss: 30.0907 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9795 - val_f1score: 0.0000e+00\n",
      "Epoch 168/200\n",
      "92/92 [==============================] - 82s 892ms/step - loss: 21.1960 - accuracy: 0.9938 - precision: 0.9918 - recall: 0.9959 - auc: 0.9991 - f1score: 0.9939 - val_loss: 29.4576 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9782 - val_f1score: 0.0000e+00\n",
      "Epoch 169/200\n",
      "92/92 [==============================] - 82s 892ms/step - loss: 21.0230 - accuracy: 0.9932 - precision: 0.9911 - recall: 0.9952 - auc: 0.9984 - f1score: 0.9932 - val_loss: 29.1343 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9779 - val_f1score: 0.0000e+00\n",
      "Epoch 170/200\n",
      "92/92 [==============================] - 82s 893ms/step - loss: 20.7923 - accuracy: 0.9935 - precision: 0.9911 - recall: 0.9959 - auc: 0.9988 - f1score: 0.9935 - val_loss: 28.8151 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9771 - val_f1score: 0.0000e+00\n",
      "Epoch 171/200\n",
      "92/92 [==============================] - 82s 891ms/step - loss: 20.6347 - accuracy: 0.9935 - precision: 0.9925 - recall: 0.9945 - auc: 0.9990 - f1score: 0.9935 - val_loss: 28.7054 - val_accuracy: 0.9452 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9691 - val_f1score: 0.0000e+00\n",
      "Epoch 172/200\n",
      "92/92 [==============================] - 82s 892ms/step - loss: 20.4853 - accuracy: 0.9911 - precision: 0.9904 - recall: 0.9918 - auc: 0.9990 - f1score: 0.9911 - val_loss: 28.5879 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9711 - val_f1score: 0.0000e+00\n",
      "Epoch 173/200\n",
      "92/92 [==============================] - 83s 897ms/step - loss: 20.4071 - accuracy: 0.9719 - precision: 0.9681 - recall: 0.9760 - auc: 0.9951 - f1score: 0.9721 - val_loss: 28.4107 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9789 - val_f1score: 0.0000e+00\n",
      "Epoch 174/200\n",
      "92/92 [==============================] - 82s 896ms/step - loss: 20.1705 - accuracy: 0.9908 - precision: 0.9911 - recall: 0.9904 - auc: 0.9994 - f1score: 0.9908 - val_loss: 28.5649 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 175/200\n",
      "92/92 [==============================] - 82s 896ms/step - loss: 19.9917 - accuracy: 0.9945 - precision: 0.9938 - recall: 0.9952 - auc: 0.9992 - f1score: 0.9945 - val_loss: 27.9812 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9720 - val_f1score: 0.0000e+00\n",
      "Epoch 176/200\n",
      "92/92 [==============================] - 82s 893ms/step - loss: 19.8177 - accuracy: 0.9979 - precision: 0.9979 - recall: 0.9979 - auc: 1.0000 - f1score: 0.9979 - val_loss: 27.8350 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9724 - val_f1score: 0.0000e+00\n",
      "Epoch 177/200\n",
      "92/92 [==============================] - 82s 889ms/step - loss: 19.6845 - accuracy: 0.9904 - precision: 0.9897 - recall: 0.9911 - auc: 0.9993 - f1score: 0.9904 - val_loss: 27.8486 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 178/200\n",
      "92/92 [==============================] - 82s 896ms/step - loss: 19.5190 - accuracy: 0.9952 - precision: 0.9952 - recall: 0.9952 - auc: 0.9996 - f1score: 0.9952 - val_loss: 27.3142 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9715 - val_f1score: 0.0000e+00\n",
      "Epoch 179/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 19.3440 - accuracy: 0.9938 - precision: 0.9918 - recall: 0.9959 - auc: 0.9995 - f1score: 0.9939 - val_loss: 27.2885 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 180/200\n",
      "92/92 [==============================] - 82s 893ms/step - loss: 19.2028 - accuracy: 0.9945 - precision: 0.9938 - recall: 0.9952 - auc: 0.9992 - f1score: 0.9945 - val_loss: 27.2461 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 181/200\n",
      "92/92 [==============================] - 82s 895ms/step - loss: 19.0217 - accuracy: 0.9969 - precision: 0.9966 - recall: 0.9973 - auc: 0.9999 - f1score: 0.9969 - val_loss: 26.8144 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9707 - val_f1score: 0.0000e+00\n",
      "Epoch 182/200\n",
      "92/92 [==============================] - 82s 892ms/step - loss: 18.9763 - accuracy: 0.9836 - precision: 0.9842 - recall: 0.9829 - auc: 0.9961 - f1score: 0.9836 - val_loss: 26.6889 - val_accuracy: 0.9521 - val_precision: 0.1667 - val_recall: 0.3333 - val_auc: 0.9668 - val_f1score: 0.2222\n",
      "Epoch 183/200\n",
      "92/92 [==============================] - 82s 888ms/step - loss: 18.7678 - accuracy: 0.9979 - precision: 0.9979 - recall: 0.9979 - auc: 1.0000 - f1score: 0.9979 - val_loss: 26.4436 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 184/200\n",
      "92/92 [==============================] - 82s 891ms/step - loss: 18.5982 - accuracy: 0.9942 - precision: 0.9945 - recall: 0.9938 - auc: 0.9992 - f1score: 0.9942 - val_loss: 26.1938 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9722 - val_f1score: 0.0000e+00\n",
      "Epoch 185/200\n",
      "92/92 [==============================] - 82s 893ms/step - loss: 18.4651 - accuracy: 0.9949 - precision: 0.9945 - recall: 0.9952 - auc: 0.9996 - f1score: 0.9949 - val_loss: 26.0164 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9717 - val_f1score: 0.0000e+00\n",
      "Epoch 186/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 18.3535 - accuracy: 0.9911 - precision: 0.9904 - recall: 0.9918 - auc: 0.9982 - f1score: 0.9911 - val_loss: 26.4277 - val_accuracy: 0.8219 - val_precision: 0.0400 - val_recall: 0.3333 - val_auc: 0.8713 - val_f1score: 0.0714\n",
      "Epoch 187/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 18.2887 - accuracy: 0.9733 - precision: 0.9714 - recall: 0.9754 - auc: 0.9947 - f1score: 0.9734 - val_loss: 25.5929 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9699 - val_f1score: 0.0000e+00\n",
      "Epoch 188/200\n",
      "92/92 [==============================] - 82s 894ms/step - loss: 18.0704 - accuracy: 0.9966 - precision: 0.9952 - recall: 0.9979 - auc: 0.9989 - f1score: 0.9966 - val_loss: 25.5750 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9778 - val_f1score: 0.0000e+00\n",
      "Epoch 189/200\n",
      "92/92 [==============================] - 82s 889ms/step - loss: 17.9345 - accuracy: 0.9952 - precision: 0.9939 - recall: 0.9966 - auc: 0.9989 - f1score: 0.9952 - val_loss: 25.3503 - val_accuracy: 0.9384 - val_precision: 0.1250 - val_recall: 0.3333 - val_auc: 0.9654 - val_f1score: 0.1818\n",
      "Epoch 190/200\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 17.8357 - accuracy: 0.9959 - precision: 0.9959 - recall: 0.9959 - auc: 0.9996 - f1score: 0.9959 - val_loss: 25.2437 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9719 - val_f1score: 0.0000e+00\n",
      "Epoch 191/200\n",
      "92/92 [==============================] - 81s 883ms/step - loss: 17.6822 - accuracy: 0.9932 - precision: 0.9925 - recall: 0.9938 - auc: 0.9984 - f1score: 0.9932 - val_loss: 25.1479 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 192/200\n",
      "92/92 [==============================] - 82s 893ms/step - loss: 17.5395 - accuracy: 0.9962 - precision: 0.9952 - recall: 0.9973 - auc: 0.9999 - f1score: 0.9962 - val_loss: 24.7216 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9699 - val_f1score: 0.0000e+00\n",
      "Epoch 193/200\n",
      "92/92 [==============================] - 81s 881ms/step - loss: 17.4341 - accuracy: 0.9928 - precision: 0.9918 - recall: 0.9938 - auc: 0.9988 - f1score: 0.9928 - val_loss: 24.7073 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9784 - val_f1score: 0.0000e+00\n",
      "Epoch 194/200\n",
      "92/92 [==============================] - 81s 883ms/step - loss: 17.2424 - accuracy: 0.9962 - precision: 0.9959 - recall: 0.9966 - auc: 0.9999 - f1score: 0.9962 - val_loss: 24.4565 - val_accuracy: 0.9589 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9757 - val_f1score: 0.0000e+00\n",
      "Epoch 195/200\n",
      "92/92 [==============================] - 82s 888ms/step - loss: 17.1624 - accuracy: 0.9942 - precision: 0.9945 - recall: 0.9938 - auc: 0.9996 - f1score: 0.9942 - val_loss: 24.4691 - val_accuracy: 0.9110 - val_precision: 0.0833 - val_recall: 0.3333 - val_auc: 0.9433 - val_f1score: 0.1333\n",
      "Epoch 196/200\n",
      "92/92 [==============================] - 82s 892ms/step - loss: 17.0554 - accuracy: 0.9928 - precision: 0.9925 - recall: 0.9932 - auc: 0.9984 - f1score: 0.9928 - val_loss: 24.4048 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9793 - val_f1score: 0.0000e+00\n",
      "Epoch 197/200\n",
      "92/92 [==============================] - 82s 889ms/step - loss: 17.0701 - accuracy: 0.9747 - precision: 0.9760 - recall: 0.9733 - auc: 0.9932 - f1score: 0.9746 - val_loss: 24.1530 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9705 - val_f1score: 0.0000e+00\n",
      "Epoch 198/200\n",
      "92/92 [==============================] - 81s 883ms/step - loss: 16.8198 - accuracy: 0.9938 - precision: 0.9932 - recall: 0.9945 - auc: 0.9995 - f1score: 0.9938 - val_loss: 23.9673 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9716 - val_f1score: 0.0000e+00\n",
      "Epoch 199/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 16.6925 - accuracy: 0.9945 - precision: 0.9945 - recall: 0.9945 - auc: 0.9995 - f1score: 0.9945 - val_loss: 23.8616 - val_accuracy: 0.9178 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9559 - val_f1score: 0.0000e+00\n",
      "Epoch 200/200\n",
      "92/92 [==============================] - 82s 890ms/step - loss: 16.5911 - accuracy: 0.9945 - precision: 0.9938 - recall: 0.9952 - auc: 0.9984 - f1score: 0.9945 - val_loss: 23.6618 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9712 - val_f1score: 0.0000e+00\n",
      "WARNING:tensorflow:From <ipython-input-1-db34cc146bdb>:145: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.evaluate, which supports generators.\n",
      "6/6 [==============================] - 5s 890ms/step - loss: 23.5090 - accuracy: 0.9835 - precision: 0.6667 - recall: 0.5000 - auc: 0.9883 - f1score: 0.5714\n",
      "loss  =  23.50901222229004\n",
      "accuracy  =  0.9835164546966553\n",
      "precision  =  0.6666666865348816\n",
      "recall  =  0.5\n",
      "auc  =  0.9882562756538391\n",
      "f1score  =  0.5714285373687744\n",
      "WARNING:tensorflow:From <ipython-input-1-db34cc146bdb>:289: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "True (-)ves:  177 \n",
      "False (+)ves:  1 \n",
      "False (-)ves:  2 \n",
      "True (+)ves:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import dill\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.summary as tf_summary\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from math import ceil\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, CategoricalAccuracy, Precision, Recall, AUC\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from models.models import *\n",
    "from visualization.visualize import *\n",
    "from custom.metrics import F1Score\n",
    "from data.preprocess import remove_text\n",
    "from pathlib import Path\n",
    "\n",
    "def get_class_weights(histogram, class_multiplier=None):\n",
    "    '''\n",
    "    Computes weights for each class to be applied in the loss function during training.\n",
    "    :param histogram: A list depicting the number of each item in different class\n",
    "    :param class_multiplier: List of values to multiply the calculated class weights by. For further control of class weighting.\n",
    "    :return: A dictionary containing weights for each class\n",
    "    '''\n",
    "    weights = [None] * len(histogram)\n",
    "    for i in range(len(histogram)):\n",
    "        weights[i] = (1.0 / len(histogram)) * sum(histogram) / histogram[i]\n",
    "    class_weight = {i: weights[i] for i in range(len(histogram))}\n",
    "    if class_multiplier is not None:\n",
    "        class_weight = {i: class_weight[i] * class_multiplier[i] for i in range(len(histogram))} # changed from list to dict by addin '{i: }'\n",
    "    print(\"Class weights: \", class_weight)\n",
    "    return class_weight\n",
    "\n",
    "\n",
    "def random_minority_oversample(train_set):\n",
    "    '''\n",
    "    Oversample the minority class using the specified algorithm\n",
    "    :param train_set: Training set image file names and labels\n",
    "    :return: A new training set containing oversampled examples\n",
    "    '''\n",
    "    X_train = train_set[[x for x in train_set.columns if x != 'label']].to_numpy()\n",
    "    if X_train.shape[1] == 1:\n",
    "        X_train = np.expand_dims(X_train, axis=-1)\n",
    "    Y_train = train_set['label'].to_numpy()\n",
    "    sampler = RandomOverSampler(random_state=np.random.randint(0, high=1000))\n",
    "    X_resampled, Y_resampled = sampler.fit_resample(X_train, Y_train)\n",
    "    filenames = X_resampled[:, 1]     # Filename is in second column\n",
    "    label_strs = X_resampled[:, 2]    # Class name is in second column\n",
    "    print(\"Train set shape before oversampling: \", X_train.shape, \" Train set shape after resampling: \", X_resampled.shape)\n",
    "    train_set_resampled = pd.DataFrame({'filename': filenames, 'label': Y_resampled, 'label_str': label_strs})\n",
    "    return train_set_resampled\n",
    "\n",
    "\n",
    "def train_model(cfg, data, callbacks, verbose=1):\n",
    "    '''\n",
    "    Train a and evaluate model on given data.\n",
    "    :param cfg: Project config (from config.yml)\n",
    "    :param data: dict of partitioned dataset\n",
    "    :param callbacks: list of callbacks for Keras model\n",
    "    :param verbose: Verbosity mode to pass to model.fit_generator()\n",
    "    :return: Trained model and associated performance metrics on the test set\n",
    "    '''\n",
    "\n",
    "    # If set in config file, oversample the minority class\n",
    "    if cfg['TRAIN']['IMB_STRATEGY'] == 'random_oversample':\n",
    "        data['TRAIN'] = random_minority_oversample(data['TRAIN'])\n",
    "\n",
    "    # Create ImageDataGenerators\n",
    "    train_img_gen = ImageDataGenerator(rotation_range=10, preprocessing_function=remove_text,\n",
    "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
    "    val_img_gen = ImageDataGenerator(preprocessing_function=remove_text,\n",
    "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
    "    test_img_gen = ImageDataGenerator(preprocessing_function=remove_text,\n",
    "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
    "\n",
    "    # Create DataFrameIterators\n",
    "    raw_data = Path('C:/Users/PaulDS3/Downloads/project/covid_cxr/data/') # replace cfg['PATHS']['RAW_DATA']\n",
    "    out_class_ind = Path('./interpretability/output_class_indices.pkl') # replace cfg['PATHS']['OUTPUT_CLASS_INDICES']\n",
    "    img_shape = tuple(cfg['DATA']['IMG_DIM'])\n",
    "    y_col = 'label_str'\n",
    "    class_mode = 'categorical'\n",
    "    train_generator = train_img_gen.flow_from_dataframe(dataframe=data['TRAIN'], directory=cfg['PATHS']['RAW_DATA'],\n",
    "        x_col=\"filename\", y_col=y_col, target_size=img_shape, batch_size=cfg['TRAIN']['BATCH_SIZE'],\n",
    "        class_mode=class_mode, validate_filenames=False)\n",
    "    val_generator = val_img_gen.flow_from_dataframe(dataframe=data['VAL'], directory=cfg['PATHS']['RAW_DATA'],\n",
    "        x_col=\"filename\", y_col=y_col, target_size=img_shape, batch_size=cfg['TRAIN']['BATCH_SIZE'],\n",
    "        class_mode=class_mode, validate_filenames=False)\n",
    "    test_generator = test_img_gen.flow_from_dataframe(dataframe=data['TEST'], directory=cfg['PATHS']['RAW_DATA'],\n",
    "        x_col=\"filename\", y_col=y_col, target_size=img_shape, batch_size=cfg['TRAIN']['BATCH_SIZE'],\n",
    "        class_mode=class_mode, validate_filenames=False, shuffle=False)\n",
    "\n",
    "    # Save model's ordering of class indices\n",
    "    dill.dump(test_generator.class_indices, open(out_class_ind, 'wb')) # replaced cfg['PATHS']['OUTPUT_CLASS_INDICES']\n",
    "    # Apply class imbalance strategy. We have many more X-rays negative for COVID-19 than positive.\n",
    "    histogram = np.bincount(np.array(train_generator.labels).astype(int))  # Get class distribution\n",
    "    class_weight = None\n",
    "    if cfg['TRAIN']['IMB_STRATEGY'] == 'class_weight':\n",
    "        class_multiplier = cfg['TRAIN']['CLASS_MULTIPLIER']\n",
    "        class_multiplier = [class_multiplier[cfg['DATA']['CLASSES'].index(c)] for c in test_generator.class_indices]\n",
    "        class_weight = get_class_weights(histogram, class_multiplier)\n",
    "\n",
    "    # Define metrics.\n",
    "    covid_class_idx = test_generator.class_indices['COVID-19']   # Get index of COVID-19 class\n",
    "    thresholds = 1.0 / len(cfg['DATA']['CLASSES'])      # Binary classification threshold for a class\n",
    "    metrics = [CategoricalAccuracy(name='accuracy'),\n",
    "               Precision(name='precision', thresholds=thresholds, class_id=covid_class_idx),\n",
    "               Recall(name='recall', thresholds=thresholds, class_id=covid_class_idx),\n",
    "               AUC(name='auc'),\n",
    "               F1Score(name='f1score', thresholds=thresholds, class_id=covid_class_idx)]\n",
    "\n",
    "    # Define the model.\n",
    "    print('Training distribution: ', ['Class ' + list(test_generator.class_indices.keys())[i] + ': ' + str(histogram[i]) + '. '\n",
    "           for i in range(len(histogram))])\n",
    "    input_shape = cfg['DATA']['IMG_DIM'] + [3]\n",
    "    num_gpus = cfg['TRAIN']['NUM_GPUS']\n",
    "    if cfg['TRAIN']['MODEL_DEF'] == 'dcnn_resnet':\n",
    "        model_def = dcnn_resnet\n",
    "    elif cfg['TRAIN']['MODEL_DEF'] == 'resnet50v2':\n",
    "        model_def = resnet50v2\n",
    "    else:\n",
    "        model_def = resnet101v2\n",
    "    if cfg['TRAIN']['CLASS_MODE'] == 'binary':\n",
    "        histogram = np.bincount(data['TRAIN']['label'].astype(int))\n",
    "        output_bias = np.log([histogram[i] / (np.sum(histogram) - histogram[i]) for i in range(histogram.shape[0])])\n",
    "        model = model_def(cfg['NN']['DCNN_BINARY'], input_shape, metrics, 2, output_bias=output_bias, gpus=num_gpus) #removed gpus=num_gpus\n",
    "    else:\n",
    "        n_classes = len(cfg['DATA']['CLASSES'])\n",
    "        histogram = np.bincount(data['TRAIN']['label'].astype(int))\n",
    "        output_bias = np.log([histogram[i] / (np.sum(histogram) - histogram[i]) for i in range(histogram.shape[0])])\n",
    "        model = model_def(cfg['NN']['DCNN_MULTICLASS'], input_shape, metrics, n_classes, output_bias=output_bias, gpus=num_gpus) #removed gpus=num_gpus\n",
    "\n",
    "    # Train the model.\n",
    "    steps_per_epoch = ceil(train_generator.n / train_generator.batch_size)\n",
    "    val_steps = ceil(val_generator.n / val_generator.batch_size)\n",
    "    history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=cfg['TRAIN']['EPOCHS'],\n",
    "                                  validation_data=val_generator, validation_steps=val_steps, callbacks=callbacks,\n",
    "                                  verbose=verbose, class_weight=class_weight)\n",
    "\n",
    "    # Run the model on the test set and print the resulting performance metrics.\n",
    "    test_results = model.evaluate_generator(test_generator, verbose=1)\n",
    "    test_metrics = {}\n",
    "    test_summary_str = [['**Metric**', '**Value**']]\n",
    "    for metric, value in zip(model.metrics_names, test_results):\n",
    "        test_metrics[metric] = value\n",
    "        print(metric, ' = ', value)\n",
    "        test_summary_str.append([metric, str(value)])\n",
    "    return model, test_metrics, test_generator\n",
    "\n",
    "\n",
    "def multi_train(cfg, data, callbacks, base_log_dir):\n",
    "    '''\n",
    "    Trains a model a series of times and returns the model with the best test set metric (specified in cfg)\n",
    "    :param cfg: Project config (from config.yml)\n",
    "    :param data: Partitioned dataset\n",
    "    :param callbacks: List of callbacks to pass to model.fit()\n",
    "    :param base_log_dir: Base directory to write logs\n",
    "    :return: The trained Keras model with best test set performance on the metric specified in cfg\n",
    "    '''\n",
    "\n",
    "    # Load order of metric preference\n",
    "    metric_preference = cfg['TRAIN']['METRIC_PREFERENCE']\n",
    "    best_metrics = dict.fromkeys(metric_preference, 0.0)\n",
    "    if 'loss' in metric_preference:\n",
    "        best_metrics['loss'] = 100000.0\n",
    "\n",
    "    # Train NUM_RUNS models and return the best one according to the preferred metrics\n",
    "    for i in range(cfg['TRAIN']['NUM_RUNS']):\n",
    "        print(\"Training run \", i+1, \" / \", cfg['TRAIN']['NUM_RUNS'])\n",
    "        cur_callbacks = callbacks.copy()\n",
    "        cur_date = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "        if base_log_dir is not None:\n",
    "            log_dir = base_log_dir + cur_date\n",
    "            cur_callbacks.append(TensorBoard(log_dir=log_dir, histogram_freq=1))\n",
    "\n",
    "        # Train the model and evaluate performance on test set\n",
    "        new_model, test_metrics, test_generator = train_model(cfg, data, cur_callbacks, verbose=1)\n",
    "\n",
    "        # Log test set results and images\n",
    "        if base_log_dir is not None:\n",
    "            log_test_results(cfg, new_model, test_generator, test_metrics, log_dir)\n",
    "\n",
    "        # If this model outperforms the previous ones based on the specified metric preferences, save this one.\n",
    "        for i in range(len(metric_preference)):\n",
    "            if (((metric_preference[i] == 'loss') and (test_metrics[metric_preference[i]] < best_metrics[metric_preference[i]]))\n",
    "                    or ((metric_preference[i] != 'loss') and (test_metrics[metric_preference[i]] > best_metrics[metric_preference[i]]))):\n",
    "                best_model = new_model\n",
    "                best_metrics = test_metrics\n",
    "                best_generator = test_generator\n",
    "                best_model_date = cur_date\n",
    "                break\n",
    "            elif (test_metrics[metric_preference[i]] == best_metrics[metric_preference[i]]):\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    print(\"Best model test metrics: \", best_metrics)\n",
    "    return best_model, best_metrics, best_generator, best_model_date\n",
    "\n",
    "\n",
    "def random_hparam_search(cfg, data, callbacks, log_dir):\n",
    "    '''\n",
    "    Conduct a random hyperparameter search over the ranges given for the hyperparameters in config.yml and log results\n",
    "    in TensorBoard. Model is trained x times for y random combinations of hyperparameters.\n",
    "    :param cfg: Project config\n",
    "    :param data: Dict containing the partitioned datasets\n",
    "    :param callbacks: List of callbacks for Keras model (excluding TensorBoard)\n",
    "    :param log_dir: Base directory in which to store logs\n",
    "    :return: (Last model trained, resultant test set metrics, test data generator)\n",
    "    '''\n",
    "\n",
    "    # Define HParam objects for each hyperparameter we wish to tune.\n",
    "    hp_ranges = cfg['HP_SEARCH']['RANGES']\n",
    "    HPARAMS = []\n",
    "    HPARAMS.append(hp.HParam('KERNEL_SIZE', hp.Discrete(hp_ranges['KERNEL_SIZE'])))\n",
    "    HPARAMS.append(hp.HParam('MAXPOOL_SIZE', hp.Discrete(hp_ranges['MAXPOOL_SIZE'])))\n",
    "    HPARAMS.append(hp.HParam('INIT_FILTERS', hp.Discrete(hp_ranges['INIT_FILTERS'])))\n",
    "    HPARAMS.append(hp.HParam('FILTER_EXP_BASE', hp.IntInterval(hp_ranges['FILTER_EXP_BASE'][0], hp_ranges['FILTER_EXP_BASE'][1])))\n",
    "    HPARAMS.append(hp.HParam('NODES_DENSE0', hp.Discrete(hp_ranges['NODES_DENSE0'])))\n",
    "    HPARAMS.append(hp.HParam('CONV_BLOCKS', hp.IntInterval(hp_ranges['CONV_BLOCKS'][0], hp_ranges['CONV_BLOCKS'][1])))\n",
    "    HPARAMS.append(hp.HParam('DROPOUT', hp.Discrete(hp_ranges['DROPOUT'])))\n",
    "    HPARAMS.append(hp.HParam('LR', hp.RealInterval(hp_ranges['LR'][0], hp_ranges['LR'][1])))\n",
    "    HPARAMS.append(hp.HParam('OPTIMIZER', hp.Discrete(hp_ranges['OPTIMIZER'])))\n",
    "    HPARAMS.append(hp.HParam('L2_LAMBDA', hp.Discrete(hp_ranges['L2_LAMBDA'])))\n",
    "    HPARAMS.append(hp.HParam('BATCH_SIZE', hp.Discrete(hp_ranges['BATCH_SIZE'])))\n",
    "    HPARAMS.append(hp.HParam('IMB_STRATEGY', hp.Discrete(hp_ranges['IMB_STRATEGY'])))\n",
    "\n",
    "    # Define test set metrics that we wish to log to TensorBoard for each training run\n",
    "    HP_METRICS = [hp.Metric(metric, display_name='Test ' + metric) for metric in cfg['HP_SEARCH']['METRICS']]\n",
    "\n",
    "    # Configure TensorBoard to log the results\n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        hp.hparams_config(hparams=HPARAMS, metrics=HP_METRICS)\n",
    "\n",
    "    # Complete a number of training runs at different hparam values and log the results.\n",
    "    repeats_per_combo = cfg['HP_SEARCH']['REPEATS']   # Number of times to train the model per combination of hparams\n",
    "    num_combos = cfg['HP_SEARCH']['COMBINATIONS']     # Number of random combinations of hparams to attempt\n",
    "    num_sessions = num_combos * repeats_per_combo       # Total number of runs in this experiment\n",
    "    model_type = 'DCNN_BINARY' if cfg['TRAIN']['CLASS_MODE'] == 'binary' else 'DCNN_MULTICLASS'\n",
    "    trial_id = 0\n",
    "    for group_idx in range(num_combos):\n",
    "        rand = random.Random()\n",
    "        HPARAMS = {h: h.domain.sample_uniform(rand) for h in HPARAMS}\n",
    "        hparams = {h.name: HPARAMS[h] for h in HPARAMS}  # To pass to model definition\n",
    "        for repeat_idx in range(repeats_per_combo):\n",
    "            trial_id += 1\n",
    "            print(\"Running training session %d/%d\" % (trial_id, num_sessions))\n",
    "            print(\"Hparam values: \", {h.name: HPARAMS[h] for h in HPARAMS})\n",
    "            trial_logdir = os.path.join(log_dir, str(trial_id))     # Need specific logdir for each trial\n",
    "            callbacks_hp = callbacks + [TensorBoard(log_dir=trial_logdir, profile_batch=0, write_graph=False)]\n",
    "\n",
    "            # Set values of hyperparameters for this run in config file.\n",
    "            for h in hparams:\n",
    "                if h in ['LR', 'L2_LAMBDA']:\n",
    "                    val = 10 ** hparams[h]      # These hyperparameters are sampled on the log scale.\n",
    "                else:\n",
    "                    val = hparams[h]\n",
    "                cfg['NN'][model_type][h] = val\n",
    "\n",
    "            # Set some hyperparameters that are not specified in model definition.\n",
    "            cfg['TRAIN']['BATCH_SIZE'] = hparams['BATCH_SIZE']\n",
    "            cfg['TRAIN']['IMB_STRATEGY'] = hparams['IMB_STRATEGY']\n",
    "\n",
    "            # Run a training session and log the performance metrics on the test set to HParams dashboard in TensorBoard\n",
    "            with tf.summary.create_file_writer(trial_logdir).as_default():\n",
    "                hp.hparams(HPARAMS, trial_id=str(trial_id))\n",
    "                model, test_metrics, test_generator = train_model(cfg, data, callbacks_hp, verbose=0)\n",
    "                for metric in HP_METRICS:\n",
    "                    if metric._tag in test_metrics:\n",
    "                        tf.summary.scalar(metric._tag, test_metrics[metric._tag], step=1)   # Log test metric\n",
    "    return\n",
    "\n",
    "\n",
    "def log_test_results(cfg, model, test_generator, test_metrics, log_dir):\n",
    "    '''\n",
    "    Visualize performance of a trained model on the test set. Optionally save the model.\n",
    "    :param cfg: Project config\n",
    "    :param model: A trained Keras model\n",
    "    :param test_generator: A Keras generator for the test set\n",
    "    :param test_metrics: Dict of test set performance metrics\n",
    "    :param log_dir: Path to write TensorBoard logs\n",
    "    '''\n",
    "\n",
    "    # Visualization of test results\n",
    "    test_predictions = model.predict_generator(test_generator, verbose=0)\n",
    "    test_labels = test_generator.labels\n",
    "    covid_idx = test_generator.class_indices['COVID-19']\n",
    "    plt = plot_roc(\"Test set\", test_labels, test_predictions, class_id=covid_idx)\n",
    "    roc_img = plot_to_tensor()\n",
    "    plt = plot_confusion_matrix(test_labels, test_predictions, class_id=covid_idx)\n",
    "    cm_img = plot_to_tensor()\n",
    "\n",
    "    # Log test set results and plots in TensorBoard\n",
    "    writer = tf_summary.create_file_writer(logdir=log_dir)\n",
    "\n",
    "    # Create table of test set metrics\n",
    "    test_summary_str = [['**Metric**','**Value**']]\n",
    "    thresholds = cfg['TRAIN']['THRESHOLDS']  # Load classification thresholds\n",
    "    for metric in test_metrics:\n",
    "        if metric in ['precision', 'recall'] and isinstance(metric, list):\n",
    "            metric_values = dict(zip(thresholds, test_metrics[metric]))\n",
    "        else:\n",
    "            metric_values = test_metrics[metric]\n",
    "        test_summary_str.append([metric, str(metric_values)])\n",
    "\n",
    "    # Create table of model and train config values\n",
    "    hparam_summary_str = [['**Variable**', '**Value**']]\n",
    "    for key in cfg['TRAIN']:\n",
    "        hparam_summary_str.append([key, str(cfg['TRAIN'][key])])\n",
    "    if cfg['TRAIN']['CLASS_MODE'] == 'binary':\n",
    "        for key in cfg['NN']['DCNN_BINARY']:\n",
    "            hparam_summary_str.append([key, str(cfg['NN']['DCNN_BINARY'][key])])\n",
    "    else:\n",
    "        for key in cfg['NN']['DCNN_BINARY']:\n",
    "            hparam_summary_str.append([key, str(cfg['NN']['DCNN_BINARY'][key])])\n",
    "\n",
    "    # Write to TensorBoard logs\n",
    "    with writer.as_default():\n",
    "        tf_summary.text(name='Test set metrics', data=tf.convert_to_tensor(test_summary_str), step=0)\n",
    "        tf_summary.text(name='Run hyperparameters', data=tf.convert_to_tensor(hparam_summary_str), step=0)\n",
    "        tf_summary.image(name='ROC Curve (Test Set)', data=roc_img, step=0)\n",
    "        tf_summary.image(name='Confusion Matrix (Test Set)', data=cm_img, step=0)\n",
    "    return\n",
    "\n",
    "def train_experiment(cfg=None, experiment='single_train', save_weights=True, write_logs=True):\n",
    "    '''\n",
    "    Defines and trains HIFIS-v2 model. Prints and logs relevant metrics.\n",
    "    :param experiment: The type of training experiment. Choices are {'single_train'}\n",
    "    :param save_weights: A flag indicating whether to save the model weights\n",
    "    :param write_logs: A flag indicating whether to write TensorBoard logs\n",
    "    :return: A dictionary of metrics on the test set\n",
    "    '''\n",
    "\n",
    "    # Load project config data\n",
    "    if cfg is None:\n",
    "        cfg = yaml.full_load(open(\"C:\\\\Users\\\\PaulDS3\\\\Downloads\\\\project\\\\covid-cxr\\\\config.yml\", 'r'))\n",
    "\n",
    "    # Set logs directory\n",
    "    cur_date = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    log_dir = cfg['PATHS']['LOGS'] + \"training\\\\\" + cur_date if write_logs else None\n",
    "    if not os.path.exists(cfg['PATHS']['LOGS'] + \"training\\\\\"):\n",
    "        os.makedirs(cfg['PATHS']['LOGS'] + \"training\\\\\")\n",
    "\n",
    "    # Load dataset file paths and labels\n",
    "    data = {}\n",
    "    data['TRAIN'] = pd.read_csv(cfg['PATHS']['TRAIN_SET'])\n",
    "    data['VAL'] = pd.read_csv(cfg['PATHS']['VAL_SET'])\n",
    "    data['TEST'] = pd.read_csv(cfg['PATHS']['TEST_SET'])\n",
    "\n",
    "    # Set callbacks.\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', verbose=1, patience=cfg['TRAIN']['PATIENCE'], mode='min', restore_best_weights=True)\n",
    "    callbacks = [early_stopping]\n",
    "\n",
    "    # Conduct the desired train experiment\n",
    "    if experiment == 'hparam_search':\n",
    "        log_dir = cfg['PATHS']['LOGS'] + \"hparam_search\\\\\" + cur_date\n",
    "        random_hparam_search(cfg, data, callbacks, log_dir)\n",
    "    else:\n",
    "        if experiment == 'multi_train':\n",
    "            base_log_dir = cfg['PATHS']['LOGS'] + \"training\\\\\" if write_logs else None\n",
    "            model, test_metrics, test_generator, cur_date = multi_train(cfg, data, callbacks, base_log_dir)\n",
    "        else:\n",
    "            if write_logs:\n",
    "                tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "                callbacks.append(tensorboard)\n",
    "            model, test_metrics, test_generator = train_model(cfg, data, callbacks)\n",
    "            if write_logs:\n",
    "                log_test_results(cfg, model, test_generator, test_metrics, log_dir)\n",
    "        if save_weights:\n",
    "            model_path = cfg['PATHS']['MODEL_WEIGHTS'] + 'model' + cur_date + '.h5'\n",
    "            save_model(model, model_path)  # Save the model's weights\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cfg = yaml.full_load(open(\"C:\\\\Users\\\\PaulDS3\\\\Downloads\\\\project\\\\covid-cxr\\\\config.yml\", 'r'))\n",
    "    train_experiment(cfg=cfg, experiment=cfg['TRAIN']['EXPERIMENT_TYPE'], save_weights=True, write_logs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
